# Image-Captioning-with-PyTorch
This project was to built an Encoder-Decoder model to find patterns in images and then use that information to automatically generate a description captions. A pretrained CLIP model was applied, along with data augmentation to further boost the performance. It showed competitive results on MS COCO data.
